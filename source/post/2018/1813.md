---
author: owent
categories:
  - Article
  - Blablabla
date: 2018-12-21 22:49:50
draft: true
id: 1813
tags: 
  - ELK
  - elasticsearch
  - logstash
  - kibana
title: Google去中性化分布式系统论文三件套(Percolator、Spanner、F1)读后感
type: post
---

前言
================================================

之前看过 [《大规模分布式存储系统：原理解析与架构实战》][1] ，这个系统设计还是挺有意思的，里面提及了Google的一整套系统都有论文，而且现在已经进化到下一代支持分布式跨行事务的关系型数据库系统了。所以一直很想抽时间看看Google的那套去中心化并且可以平行扩容的分布式系统和数据库的论文。之前一些计划中的我自己的项目的优化项都差不多完成了，这段时间就陆陆续续的看完了这三篇Paper，可怜我的渣渣英语，所以看得比较慢。

Google的分布式系统基础组件的层层迭代真的是做的很好。在假定所有节点都可能出故障,并且任何节点出故障都不影响可用性的基础上，先是实现了分布式PB
级别的文件系统 **GFS**[^gfs] 。然后在此基础上，实现了支持海量数据并且最终一致性并且支持自动按列分组和分片的KV存储系统 **Bigtable**[^bigtable] 。而我准备看的这三篇文章分别是：基于 **Bigtable**[^bigtable] 的分布式增量事务和通知系统 **Percolator**[^percolator] 、保证强一致性并且支持分布式事务和跨行事务的的大规模关系型分布式数据库系统 **Spanner**[^spanner] 和在 **Spanner**[^spanner] 的基础上实现的关系型数据库系统 **F1**[^f1]。上面所有的分布式系统都基于更早前实现的基于 **Paxos**[^paxos] 算法的分布式锁服务 **Chubby**[^chubby]。

现在业界也很多基于这些理论并在其上进行优化的数据库项目，比较知名的比如阿里的 [PolarDB][4] 和 [pingcap][2] 的 [TiDB][3] 。

基础服务
================================================


Percolator[^percolator]
================================================

**Large-scale Incremental Processing Using Distributed Transactions and Notifications** ，系统名字叫 **Percolator**[^percolator] 。其实是为了提升Google的搜索引擎索引系统效率而存在的。它的设计其实有很多局限性的。首先它主要是用于增量事务，比如说搜索引擎中扫描到页面中某一个部分（比如blog里的推荐阅读列表）发生变化了，其他内容其实不需要重新算权重。那么整个页面的权重可能只要重算这个变化的区域+页面宗权重即可。另外一个特性就是延迟不敏感，因为 **Percolator**[^percolator] 里很多设计都是高延时的（茫茫多的锁和冲突时Waiting），所以要求应用不太在意延时。也是比如搜索引擎里，过几分钟再建索引并不是大问题。


## 事务支持

**Percolator**[^percolator] 的事务依赖 **Bigtable**[^bigtable] 时间戳的多版本数据保存和事务。每个Value都绑定了一系列元数据列，写入到 **Bigtable**[^bigtable] 的同一个本地组（Locality group）里。因为一个 **Locality group** 的物理部署在同一组 **Bigtable**[^bigtable] 节点上，这样可以实现对同一个 **Locality group** 的多列进行原子操作，也能加快关联数据的查找速度。

## 主要逻辑代码
```cpp
class Transaction {
    struct Write { Row row; Column col; string value; };
    vector<Write> writes ;
    int start_ts ;
    Transaction() : start_ts (oracle.GetTimestamp()) {}
    void Set(Write w) { writes .push back(w); }
    bool Get(Row row, Column c, string* value) {
        while (true) {
            bigtable::Txn T = bigtable::StartRowTransaction(row);
            // Check for locks that signal concurrent writes.
            if (T.Read(row, c+"lock", [0, start_ts ])) {
                // There is a pending lock; try to clean it and wait
                BackoffAndMaybeCleanupLock(row, c);
                continue;
            }
            // Find the latest write below our start timestamp.
            latest_write = T.Read(row, c+"write", [0, start_ts ]);
            if (!latest_write.found()) 
                return false; // no data
            int data_ts = latest_write.start_timestamp();
            *value = T.Read(row, c+"data", [data_ts, data_ts]);
            return true;
        }
    }

    // Prewrite tries to lock cell w, returning false in case of conflict.
    bool Prewrite(Write w, Write primary) {
        Column c = w.col;
        bigtable::Txn T = bigtable::StartRowTransaction(w.row);
        // Abort on writes after our start timestamp . . .
        if (T.Read(w.row, c+"write", [start_ts , ∞])) 
            return false;
        // . . . or locks at any timestamp.
        if (T.Read(w.row, c+"lock", [0, ∞])) 
            return false;
        T.Write(w.row, c+"data", start_ts , w.value);
        T.Write(w.row, c+"lock", start_ts , {primary.row, primary.col});
        // The primary’s location.
        return T.Commit();
    }

    bool Commit() {
        Write primary = writes [0];
        vector<Write> secondaries(writes .begin()+1, writes .end());
        if (!Prewrite(primary, primary)) 
            return false;
        for (Write w : secondaries)
            if (!Prewrite(w, primary)) 
                return false;
        int commit_ts = oracle .GetTimestamp();
        // Commit primary first.
        Write p = primary;
        bigtable::Txn T = bigtable::StartRowTransaction(p.row);
        if (!T.Read(p.row, p.col+"lock", [start_ts , start_ts ]))
            return false;
        // aborted while working
        T.Write(p.row, p.col+"write", commit_ts, start_ts ); // Pointer to data written at start_ts .
        T.Erase(p.row, p.col+"lock", commit_ts);

        if (!T.Commit()) 
            return false;
        // commit point
        // Second phase: write out write records for secondary cells.
        for (Write w : secondaries) {
            bigtable::Write(w.row, w.col+"write", commit_ts, start_ts );
            bigtable::Erase(w.row, w.col+"lock", commit_ts);
        }
        return true;
    }
} // class Transaction
```

## 写事务提交流程

1. 选取一个待写入Key作为主提交主键
  > 两阶段提交时以这个主键的锁为准。因为在预提交阶段所有相关Key都会被锁，所以所有这个事务加锁的时候会把这个主键的锁写入锁。在故障恢复阶段如果发现数据被锁了，就检查这个写入的主键是否以解锁，如果以解锁说明事务完成了，直接删除自己的锁，预提交阶段写入的数据生效。否则事务被放弃走数据恢复流程。

2. 预提交(Prewrite): 此时数据已写入但不可被读
  + 检查可写时间戳（版本号）
  + 检查锁时间段（版本号）
  + 写入数据
  + 写入锁

3. 分配一个解锁事务 **提交时间戳（版本号,commit_ts）**
  > 复查主键的锁上的事务时间戳（版本号），这里是为了实现原子的CAS操作。用于处理其他Worker节点认为本次事务卡死而清理它导致的一致性问题。
  > 由于 **Bigtable**[^bigtable] 是最终一致性的，所以这里保证多个事务处理一个
4. 主键解锁，版本号设为 **提交时间戳（版本号,commit_ts）** ，这个值肯定比 **事务时间戳（版本号,start_ts）** 大
5. 所有其他Key解锁，并把版本号设为 版本号设为 **提交时间戳（版本号,commit_ts）**

## 读事务和故障恢复流程

1. 读取制定时间戳（版本号）前的最后的锁
  + 如果被锁了可能是需要等待，也可能需要走故障恢复流程来强制解锁
2. 读取制定时间戳（版本号）前的最后一次提交的预提交时间戳（版本号）。
  > 因为数据保存在预提交中，数据最后提交后会把write字段写为那次事务的预提交时间戳（版本号）
3. 按时间戳（版本号）获取该Key的数据

## 关于冲突的细节

上面所有的操作都基于 **Bigtable**[^bigtable] 的按版本号记录数据的功能，数据更新和加解锁都是标记一个更新的时间修改的某个数据。这和 **GFS**[^gfs] 和 **Bigtable**[^bigtable] 只有Add和Update，没有Delete的特性有关，具体的详情 [《大规模分布式存储系统：原理解析与架构实战》][1]  和 这两个组件的Paper里都有，这里就不复数原理和原因了。

锁信息要记录事务的Worker节点。每个Key只可能有一个未完成的事务，这时候lock信息会记录关联的主键 ；Worker节点会不断地写入自己的保活信息到 **Chubby** [^chubby] 中（原文中叫分布式锁服务） ；在故障恢复流程中，如果保活信息（锁）丢失或超时了，则是Worker异常崩溃或卡死了，可以直接认为事务失败。

如果读取的时候发现相关联的主键已经写入更高版本的写记录了，那么说明事务完成了，但是最后一个阶段解锁失败。这时候直接用更新的时间戳解锁即可。

如果读取的时候发现相关联的主键还没有解锁，说明事务异常终止了，这时候要走数据恢复流程。 Paper里说数据回滚是一个高消耗操作，我的理解是因为一方面这种情况得去获取 **Chubby** [^chubby] 锁信息，而这种 **Paxos**[^paxos] 操作流程其实性能还蛮受限的，而且延时可能高。另一方面 **Bigtable**[^bigtable] 正常读取数据流程的话数据不会太久远，不需要读取很长时间以前的快照和读写记录，而数据恢复的时候就不一定了，因为是Get的时候去尝试恢复的，所以可能是很久之前的信息。当然 **Percolator**[^percolator] 有定期扫描服务会把这种周期控制在一个可控范围内。

## 时间戳服务

时间戳服务Paper里说支持百万级QPS的分配版本号的服务。原文叫"Timestamp oracle"服务，直译是时间戳，但是我的理解更像是分配版本号的服务。它每次都分配一个ID段区间段并写入落地，然后如果每次不够都再分配一个段。如果这个服务节点崩溃，下次启动时会重新分配一个段，以此来保证严格递增。和我以前写的 [全局ID分配的RPC接口](https://github.com/atframework/atsf4g-co/blob/sample_solution/src/server_frame/rpc/db/uuid.cpp) 有点像。Google这个服务说是有单机 200W的QPS。我写的这个应该还更高一点，因为他是存到关系型数据库中我是存到NoSQL中。


## 其他

关于其他的部分，其实感觉不是那么难点就不详述了。通知服务是做成了周期性检查执行，而不是按事件通知，以此来防连锁性的数据变更通知而导致性能毛刺。 Paper里也用了比较简单的设计，约定某种数据只有一个观察者（虽然功能上支持多观察者），观察者总数也很少。最终达到减小惊群效应的效果。性能方面，单机性能也一般般，当然比MySQL是好一些。

|         | Bigtable | Percolator | Relative |
|---------|----------|------------|----------|
| Read/s  | 15513    | 14590      | 0.94     |
| Write/s | 31003    | 7232       | 0.23     |


Spanner[^spanner]
================================================

**Spanner: Google's Globally-Distributed Database**[^spanner] 。

基于 **时间戳API** （后面有提到）的多版本数据库

SQL-based，支持分布式事务，支持跨行事务，支持原子更新元表

副本管理可以随着数据量增长动态分配，也可以由应用程序控制。数据在数据中心中的迁移对应用层是透明的。

两个难点:

+ 对外的读写一致性
+ 全剧读的强一致性，即某个时间点在整个集群任意节点读取数据都是一致的。

使用时间戳API对事务排序，也以此来保证外部读写一致性。

一个 **universermaster** 用于统计和调试。按物理部署的分zone，每个zone一个 **zonemaster** 用于分组管理和负载均衡，多个 **location proxy** 用于和客户端通信，成百上千个 **spanserver** 用于处理逻辑。

![Spanner 结构](1813-01.png)


按 Key+时间戳 来作为 **Spanner**[^spanner] 的Key，并以此来划分 **tablet** （类似 **Bigtable**[^bigtable] 的 **tablet** ）。

每个 **spanserver** 都服务一部分 **tablet** ，具体的负载均衡和副本管理由上层 **zonemaster** 决定。每个 **tablet** 有自己的 **Paxos**[^paxos] 组（一组 **tablet** 的主备一起组成一个 **Paxos group**），并以此来选出来哪个 **spanserver** 是主节点。这意味着在 **Spanner**[^spanner] 中， **Paxos**[^paxos] 的数据分组数量会非常多。 不过 **Paxos**[^paxos] 仅仅是用来选主的，具体分片信息是存储在主节点的 **tablet** 里。 

数据按桶组织在 **directory** 里，个人理解有点像 **redis-cluster**[^redis] 里的 **slot** ，里面包含了一系列的 **tablet** 。这也是 **Spanner**[^spanner] 数据管理和迁移的单位。 即便在事务执行过程中 **directory** 也可以被迁移（ 原文中叫 **Movedir** 任务）。 对于不跨 **directory** 的事务，主节点直接执行就行了，对于跨 **directory** 的事务，则需要选举出一个 **spanserver** 作为事务处理主节点。 **spanserver** 里有专门的 **transaction manager** 来处理这一类事情。

**Movedir** 任务迁移数据的时候是每次锁两个相关的 **Paxos group** 然后执行数据迁移。这样即便事务执行时间比较长的时候，也不会因为大量锁数据而导致大量的数据分片服务暂停。

每个 **tablet** 的数据分组里都存了一行的主键，支持“继承”功能（其实是关联，相当于告诉 **Spanner**[^spanner] 多个表的关系，以便 **Spanner**[^spanner] 把他们尽量放一起，减小跨 **tablet** 的可能性）。 顺带还支持关联删除。

### 时间戳API（TrueTime API）

在大规模集群中，不同机器间必然会有对时的时间抖动导致一个不确定的时间窗，Google使用了GPS和原子钟的技术让这个不确定的时间窗缩小到了10ms以内。这个API分配的时间是不下降的（即递增或相同）。

**TrueTime API** 设计为一个时间范围。提供这几种操作:

| Method       | Returns                              |
|--------------|--------------------------------------|
| TT.now()     | TTinterval: [earliest, latest]       |
| TT.after(t)  | true if t has definitely passed      |
| TT.before(t) | true if t has definitely not arrived |

**TrueTime API** 的对时机制采用 **GPS对时** 和 **原子钟** 两部分。并且服务上按数据中心分组，每个数据中心一个 **TrueTime API** 的主节点( **Armageddon master** )，然后每台机器都是从节点。只在主节点上启用 **GPS对时** （说是 **GPS对时** 成本较高），并且假定 **GPS对时** 无误差。 按论文里的描述，每30秒同步一次对时。 **原子钟** 的误差是每秒不超过200微妙，所以每次对时的误差不超过6毫秒。加上1毫秒的网络延迟（Google网络这么好的吗？内网延迟设计是1毫秒 -_-||），所以总误差不超过7毫秒( \\\( \epsilon \\\) )。

**TrueTime API** 的主节点之间也会定期交叉检查时间。如果误差很大则会把自己剔除出主节点。


F1[^f1]
================================================

**F1: A Distributed SQL Database That Scales**[^f1] 。

[^paxos]: https://en.wikipedia.org/wiki/Paxos_(computer_science) "Paxos"
[^chubby]: https://ai.google/research/pubs/pub27897 "The Chubby lock service for loosely-coupled distributed systems"
[^gfs]: https://ai.google/research/pubs/pub51 "The Google File System"
[^bigtable]: https://ai.google/research/pubs/pub27898 "Bigtable: A Distributed Storage System for Structured Data "
[^percolator]: https://ai.google/research/pubs/pub36726 "Large-scale Incremental Processing Using Distributed Transactions and Notifications"
[^spanner]: https://ai.google/research/pubs/pub39966 "Spanner: Google's Globally-Distributed Database"
[^f1]: https://ai.google/research/pubs/pub41344 "F1: A Distributed SQL Database That Scales"
[^redis]: https://redis.io "Redis"

[1]: https://read.douban.com/ebook/10179010/
[2]: https://pingcap.com/
[3]: https://github.com/pingcap/tidb
[4]: https://www.aliyun.com/product/polardb
