---
author: owent
categories:
  - Article
  - Blablabla
date: 2018-12-21 22:49:50
draft: true
id: 1813
tags: 
  - ELK
  - elasticsearch
  - logstash
  - kibana
title: Google去中性化分布式系统论文三件套(Percolator、Spanner、F1)读后感
type: post
---

前言
================================================

之前看过《大规模分布式系统架构与设计实战》，这个系统设计还是挺有意思的，里面提及了Google的一整套系统都有论文，而且现在已经进化到下一代支持分布式事务的关系型数据库系统了。所以一直很想抽时间看看Google的那套去中心化并且可以平行扩容的分布式系统和数据库的论文。之前一些计划中的我自己的项目的优化项都差不多完成了，这段时间就陆陆续续的看完了这三篇Paper，可怜我的渣渣英语，所以看得比较慢。

Google的分布式系统基础组件的层层迭代真的是做的很好。在假定所有节点都可能出故障,并且任何节点出故障都不影响可用性的基础上，先是实现了分布式PB
级别的文件系统 **GFS**[^gfs] 。然后在此基础上，实现了支持海量数据并且最终一致性并且支持自动按列分组和分片的KV存储系统 **Bigtable**[^bigtable] 。而我准备看的这三篇文章分别是：基于 **Bigtable**[^bigtable]的分布式增量事务和通知系统 **Percolator**[^percolator] 、保证强一致性并且支持分布式事务的的大规模关系型分布式数据库系统 **Spanner**[^spanner] 和在 **Spanner**[^spanner] 的基础上实现的关系型数据库系统 **F1**[^f1]。上面所有的分布式系统都基于更早前实现的基于 **Paxos**[^paxos] 算法的分布式锁服务 **Chubby**[^chubby]。

现在业界也很多基于这些理论并在其上进行优化的数据库项目，比较知名的比如阿里的 [PolarDB][4] 和 [pingcap][2] 的 [TiDB][3] 。

基础服务
================================================

Timestamp oracle服务: 支持百万级QPS的分配版本号的服务。原文叫"Timestamp oracle"服务，直译是时间戳，但是我的理解更像是分配版本号的服务。它每次都分配一个ID段区间段并写入落地，然后如果每次不够都再分配一个段。如果这个服务节点崩溃，下次启动时会重新分配一个段，以此来保证严格递增。和我以前写的 [全局ID分配的RPC接口](https://github.com/atframework/atsf4g-co/blob/sample_solution/src/server_frame/rpc/db/uuid.cpp) 有点像。Google这个服务说是有单机 200W的QPS。我写的这个应该还更高一点，因为他是存到关系型数据库中我是存到NoSQL中。

时间戳API（TrueTime API）：在大规模集群中，不同机器间必然会有对时的时间抖动导致一个不确定的时间窗，Google使用了GPS和原子钟的技术让这个不确定的时间窗缩小到了10ms以内。这个API分配的时间是不下降的（即递增或相同）。

Percolator[^percolator]
================================================

**Large-scale Incremental Processing Using Distributed Transactions and Notifications**[^percolator] 。

搜索引擎索引系统

增量事务，不在意延迟

通知机制+错峰执行+假定某种数据只有一个观察者

依赖Bigtable时间戳的多版本数据保存

Timestamp oracle服务

## 事务支持
每个Value都绑定了一系列元数据列，写入到Bigtable的同一个本地组（Locality group）里。因为一个 **Locality group** 的物理部署在同一组Bigtable节点上，这样可以实现对同一个 **Locality group** 的多列进行原子操作，也能加快关联数据的查找速度。

## 主要逻辑代码
```cpp
class Transaction {
    struct Write { Row row; Column col; string value; };
    vector<Write> writes ;
    int start_ts ;
    Transaction() : start_ts (oracle.GetTimestamp()) {}
    void Set(Write w) { writes .push back(w); }
    bool Get(Row row, Column c, string* value) {
        while (true) {
            bigtable::Txn T = bigtable::StartRowTransaction(row);
            // Check for locks that signal concurrent writes.
            if (T.Read(row, c+"lock", [0, start_ts ])) {
                // There is a pending lock; try to clean it and wait
                BackoffAndMaybeCleanupLock(row, c);
                continue;
            }
            // Find the latest write below our start timestamp.
            latest write = T.Read(row, c+"write", [0, start_ts ]);
            if (!latest write.found()) 
                return false; // no data
            int data ts = latest write.start timestamp();
            *value = T.Read(row, c+"data", [data ts, data ts]);
            return true;
        }
    }

    // Prewrite tries to lock cell w, returning false in case of conflict.
    bool Prewrite(Write w, Write primary) {
        Column c = w.col;
        bigtable::Txn T = bigtable::StartRowTransaction(w.row);
        //
        Abort on writes after our start timestamp . . .
        if (T.Read(w.row, c+"write", [start_ts , ∞])) 
            return false;
        //
        . . . or locks at any timestamp.
        if (T.Read(w.row, c+"lock", [0, ∞])) 
            return false;
        T.Write(w.row, c+"data", start_ts , w.value);
        T.Write(w.row, c+"lock", start_ts , {primary.row, primary.col});
        // The primary’s location.
        return T.Commit();
    }

    bool Commit() {
        Write primary = writes [0];
        vector<Write> secondaries(writes .begin()+1, writes .end());
        if (!Prewrite(primary, primary)) 
            return false;
        for (Write w : secondaries)
            if (!Prewrite(w, primary)) 
                return false;
        int commit_ts = oracle .GetTimestamp();
        // Commit primary first.
        Write p = primary;
        bigtable::Txn T = bigtable::StartRowTransaction(p.row);
        if (!T.Read(p.row, p.col+"lock", [start_ts , start_ts ]))
            return false;
        // aborted while working
        T.Write(p.row, p.col+"write", commit_ts, start_ts ); // Pointer to data written at start_ts .
        T.Erase(p.row, p.col+"lock", commit_ts);

        if (!T.Commit()) 
            return false;
        // commit point
        // Second phase: write out write records for secondary cells.
        for (Write w : secondaries) {
            bigtable::Write(w.row, w.col+"write", commit_ts, start_ts );
            bigtable::Erase(w.row, w.col+"lock", commit_ts);
        }
        return true;
    }
} // class Transaction
```

## 写事务提交流程

1. 选取一个待写入Key作为主提交主键
  > 两阶段提交时以这个主键的锁为准。因为在预提交阶段所有相关Key都会被锁，所以所有这个事务加锁的时候会把这个主键的锁写入锁。在故障恢复阶段如果发现数据被锁了，就检查这个写入的主键是否以解锁，如果以解锁说明事务完成了，直接删除自己的锁，预提交阶段写入的数据生效。否则事务被放弃走数据恢复流程。

2. 预提交(Prewrite): 此时数据已写入但不可被读
  + 检查可写时间戳（版本号）
  + 检查锁时间段（版本号）
  + 写入数据
  + 写入锁

3. 分配一个解锁事务 **提交时间戳（版本号,commit_ts）**
  + 复查主键的锁上的事务时间戳（版本号），这里是为了实现原子的CAS操作
  > 由于Bigtable是最终一致性的，所以这里保证多个事务处理一个
5. 主键解锁，版本号设为 **提交时间戳（版本号,commit_ts）** ，这个值肯定比 **事务时间戳（版本号,start_ts）** 大
6. 所有其他Key解锁，并把版本号设为 版本号设为 **提交时间戳（版本号,commit_ts）**

### 关于冲突的细节

+ 锁信息要记录事务的Worker节点
  * 每个Key只可能有一个未完成的事务，这时候lock信息会记录关联的主键 ；
  * Worker节点会不断地写入自己的保活信息到 **Chubby** [^chubby] 中（原文中叫分布式锁服务） ；
  * 在故障恢复流程中，如果保活信息（锁）超时了，则是Worker异常崩溃了，可以直接认为事务失败 ；

## 读事务和故障恢复流程

Spanner[^spanner]
================================================

**Spanner: Google's Globally-Distributed Database**[^spanner] 。

基于时间戳API的多版本数据库

SQL-based，支持分布式事务，支持原子更新元表

副本管理可以随着数据量增长动态分配，也可以由应用程序控制。数据在数据中心中的迁移对应用层是透明的。

两个难点:

+ 对外的读写一致性
+ 全剧读的强一致性，即某个时间点在整个集群任意节点读取数据都是一致的。

使用时间戳API对事务排序，也以此来保证外部读写一致性。

一个 **universermaster** 用于统计和调试。按物理部署的分zone，每个zone一个 **zonemaster** 用于分组管理和负载均衡，多个 **location proxy** 用于和客户端通信，成百上千个 **spanserver** 用于处理逻辑。

![Spanner 结构](1813-01.png)


F1[^f1]
================================================

**F1: A Distributed SQL Database That Scales**[^f1] 。

[^paxos]: https://en.wikipedia.org/wiki/Paxos_(computer_science) "Paxos"
[^chubby]: https://ai.google/research/pubs/pub27897 "The Chubby lock service for loosely-coupled distributed systems"
[^gfs]: https://ai.google/research/pubs/pub51 "The Google File System"
[^bigtable]: https://ai.google/research/pubs/pub27898 "Bigtable: A Distributed Storage System for Structured Data "
[^percolator]: https://ai.google/research/pubs/pub36726 "Large-scale Incremental Processing Using Distributed Transactions and Notifications"
[^spanner]: https://ai.google/research/pubs/pub39966 "Spanner: Google's Globally-Distributed Database"
[^f1]: https://ai.google/research/pubs/pub41344 "F1: A Distributed SQL Database That Scales"

[1]: https://book.douban.com/subject/25843316/
[2]: https://pingcap.com/
[3]: https://github.com/pingcap/tidb
[4]: https://www.aliyun.com/product/polardb